{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a DQN algorithm from Scratch for Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sandeep/miniconda3/envs/rlenv/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See that, we are using the correct environment\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code modified from: https://github.com/keon/deep-q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import keras.backend as k\n",
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(1)\n",
    "env.seed(1)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "input_shape = env.observation_space.shape[0]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=input_shape, activation='relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=0.001))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "class SimpleAgent:\n",
    "    def __init__(self, action_size, model,exp=1.0):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.99    # discount factor\n",
    "        \n",
    "        self.epsilon = exp  # exploration rate\n",
    "        \n",
    "        self.epsilon_min = 0.01 # We always do some exploration\n",
    "        \n",
    "        self.epsilon_decay = 0.999\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def buffer(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def get_act(self, state):\n",
    "        #Return Random action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    #This is where training happens\n",
    "    def replay(self, batch_size):\n",
    "        \n",
    "        #Get a Random Minibatch\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "              target = reward + self.gamma * \\\n",
    "                       np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            \n",
    "            #only update the target for action taken\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gym environment and the agent\n",
    "env = gym.make('CartPole-v0')\n",
    "nb_actions = env.action_space.n\n",
    "agent = SimpleAgent(nb_actions,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Trained_Scratch_Cartpole.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the Model: Let us Play!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/20, steps: 19, exploration: 1.0\n",
      "episode: 1/20, steps: 15, exploration: 1.0\n",
      "episode: 2/20, steps: 17, exploration: 1.0\n",
      "episode: 3/20, steps: 25, exploration: 0.99\n",
      "episode: 4/20, steps: 18, exploration: 0.97\n",
      "episode: 5/20, steps: 17, exploration: 0.95\n",
      "episode: 6/20, steps: 33, exploration: 0.92\n",
      "episode: 7/20, steps: 12, exploration: 0.91\n",
      "episode: 8/20, steps: 15, exploration: 0.9\n",
      "episode: 9/20, steps: 43, exploration: 0.86\n",
      "episode: 10/20, steps: 34, exploration: 0.83\n",
      "episode: 11/20, steps: 13, exploration: 0.82\n",
      "episode: 12/20, steps: 79, exploration: 0.76\n",
      "episode: 13/20, steps: 147, exploration: 0.65\n",
      "episode: 14/20, steps: 25, exploration: 0.64\n",
      "episode: 15/20, steps: 89, exploration: 0.58\n",
      "episode: 16/20, steps: 65, exploration: 0.55\n",
      "episode: 17/20, steps: 80, exploration: 0.5\n",
      "episode: 18/20, steps: 22, exploration: 0.49\n",
      "episode: 19/20, steps: 174, exploration: 0.41\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "batch_size = 64\n",
    "num_episodes = 20\n",
    "max_steps = 200\n",
    "\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, input_shape]) #Reshape for later NN Training\n",
    "    for steps in range(max_steps):\n",
    "        #Visualization of the env\n",
    "        #env.render()\n",
    "\n",
    "        #Get the action from the Agent\n",
    "        action = agent.get_act(state)\n",
    "        \n",
    "        #Take this action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.buffer(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, steps: {}, exploration: {:.2}\"\n",
    "                  .format(e+1, num_episodes, steps, agent.epsilon))\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"Trained_Scratch_Cartpole.h5f\", overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_episode: 1/5, steps: 200, exploration: 0.0\n",
      "test_episode: 2/5, steps: 200, exploration: 0.0\n",
      "test_episode: 3/5, steps: 200, exploration: 0.0\n",
      "test_episode: 4/5, steps: 200, exploration: 0.0\n",
      "test_episode: 5/5, steps: 200, exploration: 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "test_episode= 5\n",
    "model.load_weights('Trained_Scratch_Cartpole.h5f')\n",
    "\n",
    "agent = SimpleAgent(nb_actions,model,exp=0.0)\n",
    "\n",
    "for e in range(test_episode):\n",
    "    state = env.reset()\n",
    "    \n",
    "    for steps in range(200):\n",
    "        \n",
    "        state = np.reshape(state, [1, input_shape]) #Reshape for later NN input\n",
    "        \n",
    "        #Visualization of the env\n",
    "        env.render()\n",
    "\n",
    "        #Get the action from the Agent\n",
    "        action = agent.get_act(state)\n",
    "        \n",
    "        #Take this action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print(\"test_episode: {}/{}, steps: {}, exploration: {:.2}\"\n",
    "                  .format(e+1, test_episode, steps+1, agent.epsilon))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
